{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxyUIwx2x+lgnRPAt1kvwr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kidais-lab/Mad-Libs-Generator/blob/master/Image_Segmentation_using_Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Image Segmentation using Deep Learning**"
      ],
      "metadata": {
        "id": "YupNW6SynSMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**In this project, we implemented an image segmentation model using Convolutional Neural Networks (CNNs) with TensorFlow and Keras. The purpose of this project is to segment images from the Oxford-IIIT Pet dataset, which involves classifying each pixel in the image into one of several classes (e.g., pet, background).**"
      ],
      "metadata": {
        "id": "h5YMejNHnr92"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This project demonstrates the application of deep learning techniques for image segmentation. By following the steps outlined, we successfully built and trained a CNN model capable of segmenting images from the Oxford-IIIT Pet dataset. The model's performance was evaluated using visualizations of the predicted masks, showcasing its ability to accurately classify pixels in the images"
      ],
      "metadata": {
        "id": "wx-sfcjNnzu2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Downloading and Extracting the Dataset"
      ],
      "metadata": {
        "id": "GuMgrURUn6ON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step downloads the Oxford-IIIT Pet dataset, which consists of images of pets and their annotations for segmentation tasks. The dataset is downloaded in compressed tar.gz format and extracted for further use."
      ],
      "metadata": {
        "id": "wFQDRnNMoQyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the images and annotations datasets from the Oxford Pets dataset\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
        "!wget http://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
        "\n",
        "# Extract the downloaded tar.gz files\n",
        "!tar -xf images.tar.gz\n",
        "!tar -xf annotations.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GshXyPuwoW7E",
        "outputId": "fed95191-113f-44f1-857a-a6d274644116"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-15 10:45:07--  http://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz [following]\n",
            "--2024-05-15 10:45:08--  https://www.robots.ox.ac.uk/~vgg/data/pets/data/images.tar.gz\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz [following]\n",
            "--2024-05-15 10:45:08--  https://thor.robots.ox.ac.uk/~vgg/data/pets/images.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz [following]\n",
            "--2024-05-15 10:45:09--  https://thor.robots.ox.ac.uk/datasets/pets/images.tar.gz\n",
            "Reusing existing connection to thor.robots.ox.ac.uk:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 791918971 (755M) [application/octet-stream]\n",
            "Saving to: ‘images.tar.gz’\n",
            "\n",
            "images.tar.gz       100%[===================>] 755.23M  29.0MB/s    in 27s     \n",
            "\n",
            "2024-05-15 10:45:36 (27.5 MB/s) - ‘images.tar.gz’ saved [791918971/791918971]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2024-05-15 10:45:36--  https://www.robots.ox.ac.uk/~vgg/data/pets/data/annotations.tar.gz\n",
            "Resolving www.robots.ox.ac.uk (www.robots.ox.ac.uk)... 129.67.94.2\n",
            "Connecting to www.robots.ox.ac.uk (www.robots.ox.ac.uk)|129.67.94.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz [following]\n",
            "--2024-05-15 10:45:37--  https://thor.robots.ox.ac.uk/~vgg/data/pets/annotations.tar.gz\n",
            "Resolving thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)... 129.67.95.98\n",
            "Connecting to thor.robots.ox.ac.uk (thor.robots.ox.ac.uk)|129.67.95.98|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz [following]\n",
            "--2024-05-15 10:45:37--  https://thor.robots.ox.ac.uk/datasets/pets/annotations.tar.gz\n",
            "Reusing existing connection to thor.robots.ox.ac.uk:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 19173078 (18M) [application/octet-stream]\n",
            "Saving to: ‘annotations.tar.gz’\n",
            "\n",
            "annotations.tar.gz  100%[===================>]  18.28M  13.4MB/s    in 1.4s    \n",
            "\n",
            "2024-05-15 10:45:39 (13.4 MB/s) - ‘annotations.tar.gz’ saved [19173078/19173078]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Preparing the Data"
      ],
      "metadata": {
        "id": "BvBPucbgolcD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step sets up the directories and loads the file paths for the input images and their corresponding target annotation images. It helps in organizing the data for easy access and further processing."
      ],
      "metadata": {
        "id": "vIDEQTFapCSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the directories for input images and target annotations\n",
        "input_dir = \"images/\"\n",
        "target_dir = \"annotations/trimaps/\"\n",
        "\n",
        "# Get sorted lists of all input image file paths and target annotation file paths\n",
        "input_img_paths = sorted(\n",
        "    [os.path.join(input_dir, fname) for fname in os.listdir(input_dir) if fname.endswith(\".jpg\")]\n",
        ")\n",
        "target_paths = sorted(\n",
        "    [os.path.join(target_dir, fname) for fname in os.listdir(target_dir) if fname.endswith(\".png\") and not fname.startswith(\".\")]\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import load_img, img_to_array\n",
        "\n",
        "# Display an example input image\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(load_img(input_img_paths[9]))"
      ],
      "metadata": {
        "id": "9qc-VsbKpFPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Displaying Target Images"
      ],
      "metadata": {
        "id": "5Z4-NM6fpJOE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step includes a function to visualize the target annotation images, which are the ground truth segmentation masks. Visualizing these masks helps in understanding the segmentation labels."
      ],
      "metadata": {
        "id": "b6gcOxXKpNHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to display target images\n",
        "def display_target(target_array):\n",
        "    # Normalize the target array values for visualization\n",
        "    normalized_array = (target_array.astype(\"uint8\") - 1) * 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(normalized_array[:, :, 0])\n",
        "\n",
        "# Load and display an example target image\n",
        "img = img_to_array(load_img(target_paths[9], color_mode=\"grayscale\"))\n",
        "display_target(img)"
      ],
      "metadata": {
        "id": "SzIZbZz6pPxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Processing and Shuffling the Data"
      ],
      "metadata": {
        "id": "7TZLlan0pSsG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, images and target masks are loaded, resized, and stored in arrays. The data is then shuffled to ensure that the training process is not biased. This step is crucial for preparing the data for model training."
      ],
      "metadata": {
        "id": "75z0CnLkpV7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Define image size and number of images\n",
        "img_size = (200, 200)\n",
        "num_imgs = len(input_img_paths)\n",
        "\n",
        "# Shuffle the input and target paths using a fixed seed for reproducibility\n",
        "random.Random(1337).shuffle(input_img_paths)\n",
        "random.Random(1337).shuffle(target_paths)\n",
        "\n",
        "# Function to load and resize input images\n",
        "def path_to_input_image(path):\n",
        "    return img_to_array(load_img(path, target_size=img_size))\n",
        "\n",
        "# Function to load and resize target images\n",
        "def path_to_target(path):\n",
        "    img = img_to_array(load_img(path, target_size=img_size, color_mode=\"grayscale\"))\n",
        "    img = img.astype(\"uint8\") - 1\n",
        "    return img\n",
        "\n",
        "# Initialize arrays to hold input images and target masks\n",
        "input_imgs = np.zeros((num_imgs,) + img_size + (3,), dtype=\"float32\")\n",
        "targets = np.zeros((num_imgs,) + img_size + (1,), dtype=\"uint8\")\n",
        "\n",
        "# Load images and targets into the arrays\n",
        "for i in range(num_imgs):\n",
        "    input_imgs[i] = path_to_input_image(input_img_paths[i])\n",
        "    targets[i] = path_to_target(target_paths[i])"
      ],
      "metadata": {
        "id": "XgTHPDMdpYu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Splitting the Data into Training and Validation Sets"
      ],
      "metadata": {
        "id": "fMwpQcRPpbs9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step splits the dataset into training and validation sets. The training set is used to train the model, while the validation set is used to evaluate the model's performance during training."
      ],
      "metadata": {
        "id": "NqRldYV1pgCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of validation samples\n",
        "num_val_samples = 1000\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_input_imgs = input_imgs[:-num_val_samples]\n",
        "train_targets = targets[:-num_val_samples]\n",
        "val_input_imgs = input_imgs[-num_val_samples:]\n",
        "val_targets = targets[-num_val_samples:]"
      ],
      "metadata": {
        "id": "jzr98dxZpg0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Building the Model"
      ],
      "metadata": {
        "id": "8H13TQOHpj8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step defines a Convolutional Neural Network (CNN) model for image segmentation. The model architecture includes several convolutional and transpose convolutional layers for downsampling and upsampling the image, respectively. The goal is to create a model that can learn to segment the images."
      ],
      "metadata": {
        "id": "YlHh17hfpmNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Function to build the model\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,))\n",
        "    x = layers.Rescaling(1./255)(inputs)\n",
        "\n",
        "    # Encoder: Convolutional layers to downsample the input image\n",
        "    x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
        "    x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "\n",
        "    # Decoder: Transpose convolutional layers to upsample back to original image size\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
        "    x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
        "\n",
        "    # Output layer with softmax activation for multi-class segmentation\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Create the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "# Build the model with the specified image size and number of classes\n",
        "model = get_model(img_size=img_size, num_classes=3)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "zuaGysXDposU",
        "outputId": "08b14afa-d5ea-4fb2-c031-41e283298d11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'img_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ceca1b139ffe>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Build the model with the specified image size and number of classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimg_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'img_size' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Compiling and Training the Model"
      ],
      "metadata": {
        "id": "_MW6Bm_8prCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step compiles the model with a specific optimizer and loss function, then trains the model using the training data while validating it on the validation data. The training process includes saving the best model based on validation performance."
      ],
      "metadata": {
        "id": "M3G05d8CpudM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model with RMSprop optimizer and sparse categorical cross-entropy loss\n",
        "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "# Define callbacks for the training process\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation.keras\", save_best_only=True)\n",
        "]\n",
        "\n",
        "# Train the model with the training data and validate with the validation data\n",
        "history = model.fit(\n",
        "    train_input_imgs, train_targets,\n",
        "    epochs=50,\n",
        "    callbacks=callbacks,\n",
        "    batch_size=64,\n",
        "    validation_data=(val_input_imgs, val_targets)\n",
        ")"
      ],
      "metadata": {
        "id": "VWiBjUlgpwec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 8: Plotting Training and Validation Loss"
      ],
      "metadata": {
        "id": "9Fu-3hfVpy88"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step plots the training and validation loss over the epochs to visualize the model's performance during training. It helps in understanding whether the model is overfitting or underfitting."
      ],
      "metadata": {
        "id": "3xAKZiu3p1U0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation loss over epochs\n",
        "epochs = range(1, len(history.history[\"loss\"]) + 1)\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "UUJ9eGDap3Os"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 9: Loading the Model and Visualizing Predictions"
      ],
      "metadata": {
        "id": "rfvmKriwp_kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This step loads the best model saved during training and uses it to make predictions on the validation data. It then visualizes the predictions to evaluate the model's segmentation performance."
      ],
      "metadata": {
        "id": "VSQM0HSJqAok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import array_to_img\n",
        "\n",
        "# Load the best model saved during training\n",
        "model = keras.models.load_model(\"oxford_segmentation.keras\")\n",
        "\n",
        "# Display a validation image and its predicted segmentation mask\n",
        "i = 4\n",
        "test_image = val_input_imgs[i]\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(array_to_img(test_image))\n",
        "\n",
        "# Predict the mask for the test image\n",
        "mask = model.predict(np.expand_dims(test_image, 0))[0]\n",
        "\n",
        "# Function to display the predicted mask\n",
        "def display_mask(pred):\n",
        "    mask = np.argmax(pred, axis=-1)\n",
        "    mask *= 127\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(mask)\n",
        "\n",
        "display_mask(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "id": "gequuX-fqCd2",
        "outputId": "d4797cc3-8b0d-4d33-f4a7-a2c738638c73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'keras' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-fda153feec83>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the best model saved during training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oxford_segmentation.keras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Display a validation image and its predicted segmentation mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'keras' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqJp6frRnNpx"
      },
      "outputs": [],
      "source": []
    }
  ]
}